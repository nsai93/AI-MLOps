{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsq3PP3/YZ+QFIMCDguvif",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsai93/AI-MLOps/blob/main/DOCKER_BASIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prerequisites**\n",
        "\n",
        "\n",
        "Install Docker: Follow the instructions on Docker's official website."
      ],
      "metadata": {
        "id": "74w-J4_GRsgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:**\n",
        "\n",
        "Create a Simple Python Application\n",
        "\n",
        "First, let's create a simple Python script.\n",
        "\n",
        "app.py\n",
        "\n",
        "print(\"Hello, Docker!\")"
      ],
      "metadata": {
        "id": "RBnH_7y_R4k1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2**\n",
        "\n",
        "Create a Dockerfile\n",
        "\n",
        "Next, create a Dockerfile in the same directory as your Python script."
      ],
      "metadata": {
        "id": "JAkAOQvVcAIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dockerfile"
      ],
      "metadata": {
        "id": "ZaA6HpATcS4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use the official Python image from the Docker Hub**\n",
        "\n",
        "FROM python:3.9-slim\n",
        "\n",
        "**Set the working directory in the container**\n",
        "\n",
        "WORKDIR /app\n",
        "\n",
        "**Copy the current directory contents into the container at /app**\n",
        "\n",
        "COPY . /app\n",
        "\n",
        "**Run the Python script**\n",
        "\n",
        "CMD [\"python\", \"app.py\"]\n"
      ],
      "metadata": {
        "id": "JR4eMPidcjwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3**\n",
        "\n",
        "Build the Docker Image"
      ],
      "metadata": {
        "id": "aG0CpVjec9uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open your terminal, navigate to the directory containing your Dockerfile and app.py, and run the following command to build your Docker image:\n",
        "\n",
        "\n",
        "docker build -t hello-docker .\n"
      ],
      "metadata": {
        "id": "F_Ozw9PGdE9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4**\n",
        "\n",
        "Run the Docker Container"
      ],
      "metadata": {
        "id": "3O2RwqDcdL3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After building the image, you can run a container based on this image using the following command:\n",
        "\n",
        "docker run hello-docker\n"
      ],
      "metadata": {
        "id": "27ED2QVYdSdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output**\n",
        "\n",
        "\n",
        "Hello, Docker!\n",
        "\n"
      ],
      "metadata": {
        "id": "uUx7FpJidZdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the Dockerfile\n",
        "\n",
        "FROM python:3.9-slim: Specifies the base image to use. Here, we're using a slim version of Python 3.9.\n",
        "\n",
        "WORKDIR /app: Sets the working directory inside the container to /app.\n",
        "\n",
        "COPY . /app: Copies the current directory contents (where your Dockerfile and app.py are located) into the /app directory inside the container.\n",
        "\n",
        "CMD [\"python\", \"app.py\"]: Specifies the command to run when the container starts. In this case, it runs the Python script app.py."
      ],
      "metadata": {
        "id": "MqpVeIqAdnhc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8H-9WeCcctY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Docker\n",
        "\n",
        "Installing Docker on Windows is a straightforward process. Docker provides a platform for developers and system administrators to develop, deploy, and run applications inside containers. Here's how you can install Docker on Windows:\n",
        "\n",
        "# 1. Check System Requirements\n",
        "Windows 10 (64-bit): Docker requires Windows 10 Pro, Enterprise, or Education with version 1903 or higher, or Windows 11.\n",
        "Windows Subsystem for Linux (WSL) 2: Docker uses WSL 2 as its backend on Windows. Ensure that WSL 2 is enabled.\n",
        "\n",
        "# 2. Enable WSL 2\n",
        "Step 1: Open PowerShell as an Administrator.\n",
        "Step 2: Run the following command to enable WSL:\n",
        "\n",
        "\n",
        "dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n",
        "\n",
        "Step 3: Enable the Virtual Machine Platform by running:\n",
        "\n",
        "dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n",
        "\n",
        "\n",
        "Step 4: Restart your computer.\n",
        "Step 5: Set WSL 2 as the default version:\n",
        "\n",
        "wsl --set-default-version 2\n",
        "\n",
        "\n",
        "Step 6: Install a Linux distribution from the Microsoft Store (e.g., Ubuntu).\n",
        "\n",
        "# 3. Download and Install Docker Desktop\n",
        "Step 1: Go to the Docker Desktop for Windows download page.\n",
        "Step 2: Click \"Get Docker\" to download the Docker Desktop installer.\n",
        "Step 3: Run the downloaded installer and follow the on-screen instructions.\n",
        "Step 4: During installation, ensure that the option \"Use the WSL 2 based engine\" is selected.\n",
        "Step 5: Once the installation is complete, click \"Close\" and restart your computer if prompted.\n",
        "\n",
        "\n",
        "# 4. Start Docker Desktop\n",
        "Step 1: After installation, Docker Desktop should start automatically. If not, you can launch it from the Start menu.\n",
        "Step 2: Docker will take a few moments to initialize.\n",
        "Step 3: Once Docker is running, you can access the Docker Dashboard by clicking the Docker icon in the system tray.\n",
        "\n",
        "\n",
        "# 5. Verify Docker Installation\n",
        "Step 1: Open a Command Prompt or PowerShell window.\n",
        "Step 2: Run the following command to verify the installation:\n",
        "\n",
        "docker --version\n",
        "Step 3: Run a test container to ensure everything is working:\n",
        "\n",
        "docker run hello-world\n",
        "Step 4: If Docker is correctly installed, you should see a \"Hello from Docker!\" message.\n",
        "\n",
        "# 6. Configure Docker Settings (Optional)\n",
        "Resource Allocation: You can adjust CPU, memory, and disk space allocation by accessing Docker Desktop settings (right-click the Docker icon in the system tray and select \"Settings\").\n",
        "Shared Drives: If you need to share drives or folders between your Windows system and Docker containers, you can configure this in the \"Resources\" section of the settings.\n",
        "\n",
        "# 7. Explore Docker\n",
        "Now that Docker is installed, you can start exploring containers, images, volumes, and networks. Docker Hub offers a vast repository of pre-built images to get you started.\n",
        "Would you like to learn more about using Docker, such as building your own Docker images or running specific applications in Docker?"
      ],
      "metadata": {
        "id": "yjn6UyBgpkBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# basic how to pull , delete\n",
        "# yaml file creation. How to built image\n",
        "0nishant93\n",
        "\n",
        "\n",
        "git config --global user.name \"techns93\"\n",
        "\n",
        "git config --global user.email \"ns.ai@outlook.com\"\n",
        "\n",
        "setp\n",
        "\n",
        "1_ go to desktop folder\n",
        "2) create folder and go inside and right click open with vscode\n",
        "3) create file and create commit.\n",
        "4) publish branch\n"
      ],
      "metadata": {
        "id": "nUN4MA48ta3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PS C:\\Users\\HP> docker run -it nginx bash\n",
        "root@ed3392c0d0f2:/# ^C\n",
        "root@ed3392c0d0f2:/# cat /etc/*release*\n",
        "PRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\n",
        "NAME=\"Debian GNU/Linux\"\n",
        "VERSION_ID=\"12\"\n",
        "VERSION=\"12 (bookworm)\"\n",
        "VERSION_CODENAME=bookworm\n",
        "ID=debian\n",
        "HOME_URL=\"https://www.debian.org/\"\n",
        "SUPPORT_URL=\"https://www.debian.org/support\"\n",
        "BUG_REPORT_URL=\"https://bugs.debian.org/\"\n",
        "root@ed3392c0d0f2:/# ^C\n",
        "root@ed3392c0d0f2:/# exit"
      ],
      "metadata": {
        "id": "h8Vmzti9FiaB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pS7AiXSDFhem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# code\n",
        "\n",
        "PS C:\\Users\\HP> docker ps\n",
        "\n",
        "CONTAINER ID   IMAGE                             COMMAND                  CREATED      STATUS      PORTS\n",
        "a20672025d09   docker/welcome-to-docker:latest   \"/docker-entrypoint.…\"   4 days ago   Up 4 days   0.0.0.0:8088->80/tc\n",
        "p   welcome-to-docker\n",
        "\n",
        "PS C:\\Users\\HP> docker ps -a\n",
        "\n",
        "CONTAINER ID   IMAGE                             COMMAND                  CREATED      STATUS      PORTS\n",
        "    NAMES\n",
        "a20672025d09   docker/welcome-to-docker:latest   \"/docker-entrypoint.…\"   4 days ago   Up 4 days   0.0.0.0:8088->80/tc\n",
        "p   welcome-to-docker\n",
        "\n",
        "PS C:\\Users\\HP> docker run nginx sleep 10\n",
        "\n",
        "PS C:\\Users\\HP> docker ps -a\n",
        "\n",
        "CONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS\n",
        "PORTS                  NAMES\n",
        "13f17f751afe   nginx                             \"/docker-entrypoint.…\"   20 seconds ago   Exited (0) 7 seconds ago   \n",
        "                       charming_curie\n",
        "a20672025d09   docker/welcome-to-docker:latest   \"/docker-entrypoint.…\"   4 days ago       Up 4 days\n",
        "\n",
        "\n",
        "PS C:\\Users\\HP> docker run -it nginx bash\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "28MK7cYjEhhr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEfkn3bPElUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have to more focus on Dev side for MLE"
      ],
      "metadata": {
        "id": "eY5xamN2IZYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Git\n",
        "\n",
        "\n",
        "git config --global user.name \"Your Name\"\n",
        "\n",
        "git config --global user.email \"your.email@example.com\"\n",
        "\n",
        "# Automation with Shell Scripts\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# Step 1: Pull the latest changes\n",
        "git pull origin main\n",
        "\n",
        "# Step 2: Create a new branch\n",
        "BRANCH_NAME=\"feature-$(date +%Y%m%d)\"\n",
        "git checkout -b $BRANCH_NAME\n",
        "\n",
        "# Step 3: Add changes\n",
        "git add .\n",
        "\n",
        "# Step 4: Commit changes with a message\n",
        "git commit -m \"Automated commit message\"\n",
        "\n",
        "# Step 5: Push the branch\n",
        "git push origin $BRANCH_NAME\n",
        "\n",
        "# Step 6: Create a pull request (optional, requires GitHub CLI)\n",
        "gh pr create --title \"Automated PR\" --body \"This is an automated pull request.\"\n"
      ],
      "metadata": {
        "id": "oRqQn235LqG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(query, document):\n",
        "    # Tokenize and convert to lowercase\n",
        "    query_tokens = query.lower().split(\" \")\n",
        "    document_tokens = document.lower().split(\" \")\n",
        "\n",
        "    # Create Counters for query and document\n",
        "    query_counter = Counter(query_tokens)\n",
        "    document_counter = Counter(document_tokens)\n",
        "\n",
        "    # Calculate dot product\n",
        "    dot_product = sum(query_counter[token] * document_counter[token] for token in query_counter.keys() & document_counter.keys())\n",
        "\n",
        "    # Calculate magnitudes\n",
        "    query_magnitude = math.sqrt(sum(query_counter[token] ** 2 for token in query_counter))\n",
        "    document_magnitude = math.sqrt(sum(document_counter[token] ** 2 for token in document_counter))\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    similarity = dot_product / (query_magnitude * document_magnitude) if query_magnitude * document_magnitude != 0 else 0\n",
        "\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "XAtxx4GKIhIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IRIS Model"
      ],
      "metadata": {
        "id": "ljXIU2aGJaFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To create a Docker image for deploying an Iris classification model, follow these steps. The image will include all necessary dependencies such as Python, scikit-learn (for the Iris model), and Flask (for serving the model through an API).**"
      ],
      "metadata": {
        "id": "nNDs5RLxJjB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Script (app.py)"
      ],
      "metadata": {
        "id": "tLllMrZxJoHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load Iris dataset and train a simple model\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'iris_model.pkl')\n",
        "\n",
        "# Initialize Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load('iris_model.pkl')\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Iris Model API is running!\"\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    prediction = model.predict([data['features']])\n",
        "    return jsonify({\"prediction\": int(prediction[0])})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "id": "yH64Mvo-Jcxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements File (requirements.txt)"
      ],
      "metadata": {
        "id": "KlUF17xdhQLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Flask==2.3.2\n",
        "scikit-learn==1.2.2\n",
        "joblib==1.2.0\n"
      ],
      "metadata": {
        "id": "xKoKuMMWhO90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dockerfile"
      ],
      "metadata": {
        "id": "koyhLfP13Lv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the official Python image from the DockerHub\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the current directory contents into the container at /app\n",
        "COPY . /app\n",
        "\n",
        "# Install any needed packages specified in requirements.txt\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Expose the Flask port\n",
        "EXPOSE 5000\n",
        "\n",
        "# Define environment variable for Flask\n",
        "ENV FLASK_ENV=development\n",
        "\n",
        "# Run the Flask app\n",
        "CMD [\"python\", \"app.py\"]\n"
      ],
      "metadata": {
        "id": "ySg-gaav3Nv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Run the Docker Image"
      ],
      "metadata": {
        "id": "_j78gDf4BEuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the Docker Image:\n",
        "\n",
        "docker build -t iris-model .\n"
      ],
      "metadata": {
        "id": "zbiv4SS7BEYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Run the Docker Container:\n",
        "docker run -p 5000:5000 iris-model\n"
      ],
      "metadata": {
        "id": "pXklEmtYBREA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the API\n",
        "curl -X POST http://localhost:5000/predict -H \"Content-Type: application/json\" -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}'\n"
      ],
      "metadata": {
        "id": "Dys7icgtBI8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UZA4ilnHBXwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Python Script (app.py)"
      ],
      "metadata": {
        "id": "nnEJQeJsTJGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import uvicorn\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Load Iris dataset and train a model\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(model, 'iris_model.pkl')\n",
        "\n",
        "# Load the saved model\n",
        "model = joblib.load('iris_model.pkl')\n",
        "\n",
        "# Define the input data model\n",
        "class Features(BaseModel):\n",
        "    features: list[float]\n",
        "\n",
        "@app.get(\"/\")\n",
        "def home():\n",
        "    return {\"message\": \"Iris Model API is running using FastAPI!\"}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(features: Features):\n",
        "    prediction = model.predict([features.features])\n",
        "    return {\"prediction\": int(prediction[0])}\n",
        "\n",
        "# To run the application locally\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "zxwcdY6jTLAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements File (requirements.txt)"
      ],
      "metadata": {
        "id": "b-TbWyrwbkqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fastapi==0.95.0\n",
        "scikit-learn==1.2.2\n",
        "joblib==1.2.0\n",
        "uvicorn==0.22.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zXnuXKFPbm2p",
        "outputId": "74acf5fe-ccc1-4504-fb9b-2c8abb69c20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-716b806c021a>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-716b806c021a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    fastapi==0.95.0\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dockerfile"
      ],
      "metadata": {
        "id": "Y-6RMeQBbu2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the official Python image from the DockerHub\n",
        "FROM python:3.9-slim\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the current directory contents into the container at /app\n",
        "COPY . /app\n",
        "\n",
        "# Install any needed packages specified in requirements.txt\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Expose the FastAPI port\n",
        "EXPOSE 8000\n",
        "\n",
        "# Command to run FastAPI app with Uvicorn\n",
        "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
      ],
      "metadata": {
        "id": "OtcxaUdwbpKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Running Locally in VS Code\n",
        "\n",
        " pip install -r requirements.txt\n",
        "python app.py\n"
      ],
      "metadata": {
        "id": "xvqLe5IxcDOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docker build -t iris-fastapi-model .\n",
        "docker run -p 8000:8000 iris-fastapi-model\n"
      ],
      "metadata": {
        "id": "4I_tABA5oZ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curl -X POST http://localhost:8000/predict -H \"Content-Type: application/json\" -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}'\n"
      ],
      "metadata": {
        "id": "RHYdlcqG0C4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The regulation technique that adds the sum of the absolute values of the coefficients as a penalty term to the loss function is called Lasso Regression.\n",
        "\n",
        "Lasso stands for Least Absolute Shrinkage and Selection Operator. In Lasso regression, the penalty term added to the loss function is the\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lasso regression tends to produce sparse models where some coefficients can become exactly zero, leading to feature selection."
      ],
      "metadata": {
        "id": "6y0BIFQta1V7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CZh2ilSTa88E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To deploy the Iris model using FastAPI instead of Flask, follow these steps. FastAPI is known for its speed and asynchronous capabilities, making it a great choice for modern APIs. Here’s the complete setup for deployment using FastAPI."
      ],
      "metadata": {
        "id": "W5riU1OApVAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "NEU8PwATpV6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False\n",
        ")"
      ],
      "metadata": {
        "id": "AcP52ve3GQAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ],
      "metadata": {
        "id": "XlkyeETnUKuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ],
      "metadata": {
        "id": "HkIzFZE_a77S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "colors_list = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence, tokenizer_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m',\n",
        "            end=' '\n",
        "        )"
      ],
      "metadata": {
        "id": "NFDOt-Ac6g-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
        "\n",
        "# Load a language model\n",
        "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer('Hello world', return_tensors='pt')\n",
        "\n",
        "# Process the tokens\n",
        "output = model(**tokens)[0]"
      ],
      "metadata": {
        "id": "8qFk9VJmMppa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        ")"
      ],
      "metadata": {
        "id": "zSItYqsTMplv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CszD2u_MphJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_generation(prompt, document, model=\"gpt-3.5-turbo-0125\"):\n",
        "    \"\"\"Generate an output based on a prompt and an input document.\"\"\"\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\":   prompt.replace(\"[DOCUMENT]\", document)\n",
        "            }\n",
        "    ]\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=messages,\n",
        "      model=model,\n",
        "      temperature=0\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "dnkUcDVeZuM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# We fit the model and extract the clusters\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=50, metric='euclidean', cluster_selection_method='eom'\n",
        ").fit(reduced_embeddings)\n",
        "clusters = hdbscan_model.labels_\n",
        "\n",
        "# How many clusters did we generate?\n",
        "len(set(clusters))"
      ],
      "metadata": {
        "id": "Rd3urNVBgmlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize topics and documents\n",
        "fig = topic_model.visualize_documents(\n",
        "    titles,\n",
        "    reduced_embeddings=reduced_embeddings,\n",
        "    width=1200,\n",
        "    hide_annotations=True\n",
        ")\n",
        "\n",
        "# Update fonts of legend for easier visualization\n",
        "fig.update_layout(font=dict(size=16))"
      ],
      "metadata": {
        "id": "HatHFXZoNmB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")"
      ],
      "metadata": {
        "id": "-n1W0hSwShOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create name and slogan for a product\n",
        "product_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a name and slogan for a chatbot that leverages LLMs.\"}\n",
        "]\n",
        "outputs = pipe(product_prompt)\n",
        "product_description = outputs[0][\"generated_text\"]\n",
        "print(product_description)"
      ],
      "metadata": {
        "id": "vtOt8f9iRYdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero-shot learning: Providing no examples\n",
        "zeroshot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": \"Create a character profile for an RPG game in JSON format.\"}\n",
        "]\n",
        "\n",
        "# Generate the output\n",
        "outputs = pipe(zeroshot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "uDdqFLqLmNa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-shot learning: Providing an example of the output structure\n",
        "one_shot_template = \"\"\"Create a short character profile for an RPG game. Make sure to only use this format:\n",
        "\n",
        "{\n",
        "  \"description\": \"A SHORT DESCRIPTION\",\n",
        "  \"name\": \"THE CHARACTER'S NAME\",\n",
        "  \"armor\": \"ONE PIECE OF ARMOR\",\n",
        "  \"weapon\": \"ONE OR MORE WEAPONS\"\n",
        "}\n",
        "\"\"\"\n",
        "one_shot_prompt = [\n",
        "    {\"role\": \"user\", \"content\": one_shot_template}\n",
        "]\n",
        "\n",
        "# Generate the output\n",
        "outputs = pipe(one_shot_prompt)\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "2nvIdsyuWZ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp.llama import Llama\n",
        "\n",
        "# Load Phi-3\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\",\n",
        "    filename=\"*fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=2048,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "sDeJ71CliHOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate output\n",
        "output = llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Create a warrior for an RPG in JSON format.\"},\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")['choices'][0]['message'][\"content\"]"
      ],
      "metadata": {
        "id": "BRMKC_fJDuoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")"
      ],
      "metadata": {
        "id": "5FdxEwzSdarK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "import string\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "    tokenized_doc = []\n",
        "    for token in text.lower().split():\n",
        "        token = token.strip(string.punctuation)\n",
        "\n",
        "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
        "            tokenized_doc.append(token)\n",
        "    return tokenized_doc"
      ],
      "metadata": {
        "id": "Pqu2x7gSpX5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_search(query, top_k=3, num_candidates=15):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "96cIzPuZBLni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_and_reranking_search(query, top_k=3, num_candidates=10):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "\n",
        "    #Add re-ranking\n",
        "    docs = [texts[hit['corpus_id']] for hit in bm25_hits]\n",
        "\n",
        "    print(f\"\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)\")\n",
        "    results = co.rerank(query=query, documents=docs, top_n=top_k, return_documents=True)\n",
        "    for hit in results.results:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.document.text.replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "VnW0Rn6DpX2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan.\n",
        "It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.\n",
        "Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind.\n",
        "\n",
        "Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.\n",
        "Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.\n",
        "Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm.\n",
        "Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.\n",
        "Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects.\n",
        "\n",
        "Interstellar premiered on October 26, 2014, in Los Angeles.\n",
        "In the United States, it was first released on film stock, expanding to venues using digital projectors.\n",
        "The film had a worldwide gross over $677 million (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014.\n",
        "It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.\n",
        "It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.\n",
        "Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\"\n",
        "\n",
        "# Split into a list of sentences\n",
        "texts = text.split('.')\n",
        "\n",
        "# Clean up to remove empty spaces and new lines\n",
        "texts = [t.strip(' \\n') for t in texts]"
      ],
      "metadata": {
        "id": "CgMhnUlipX0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def search(query, number_of_results=3):\n",
        "\n",
        "  # 1. Get the query's embedding\n",
        "  query_embed = co.embed(texts=[query],\n",
        "                input_type=\"search_query\",).embeddings[0]\n",
        "\n",
        "  # 2. Retrieve the nearest neighbors\n",
        "  distances , similar_item_ids = index.search(np.float32([query_embed]), number_of_results)\n",
        "\n",
        "  # 3. Format the results\n",
        "  texts_np = np.array(texts) # Convert texts list to numpy for easier indexing\n",
        "  results = pd.DataFrame(data={'texts': texts_np[similar_item_ids[0]],\n",
        "                              'distance': distances[0]})\n",
        "\n",
        "  # 4. Print and return the results\n",
        "  print(f\"Query:'{query}'\\nNearest neighbors:\")\n",
        "  return results"
      ],
      "metadata": {
        "id": "6r3Fty0Cowqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "import string\n",
        "\n",
        "def bm25_tokenizer(text):\n",
        "    tokenized_doc = []\n",
        "    for token in text.lower().split():\n",
        "        token = token.strip(string.punctuation)\n",
        "\n",
        "        if len(token) > 0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
        "            tokenized_doc.append(token)\n",
        "    return tokenized_doc"
      ],
      "metadata": {
        "id": "QB4l8vJqpXyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_search(query, top_k=3, num_candidates=15):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "0WqfdlCyg4cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WvaVQFX7g4OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_and_reranking_search(query, top_k=3, num_candidates=10):\n",
        "    print(\"Input question:\", query)\n",
        "\n",
        "    ##### BM25 search (lexical search) #####\n",
        "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
        "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
        "    bm25_hits = [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n]\n",
        "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "    print(f\"Top-3 lexical search (BM25) hits\")\n",
        "    for hit in bm25_hits[0:top_k]:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['corpus_id']].replace(\"\\n\", \" \")))\n",
        "\n",
        "    #Add re-ranking\n",
        "    docs = [texts[hit['corpus_id']] for hit in bm25_hits]\n",
        "\n",
        "    print(f\"\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)\")\n",
        "    results = co.rerank(query=query, documents=docs, top_n=top_k, return_documents=True)\n",
        "    for hit in results.results:\n",
        "        print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.document.text.replace(\"\\n\", \" \")))"
      ],
      "metadata": {
        "id": "aZuKVDUYtyvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"income generated\"\n",
        "\n",
        "# 1- Retrieval\n",
        "# We'll use embedding search. But ideally we'd do hybrid\n",
        "results = search(query)\n",
        "\n",
        "# 2- Grounded Generation\n",
        "docs_dict = [{'text': text} for text in results['texts']]\n",
        "response = co.chat(\n",
        "    message = query,\n",
        "    documents=docs_dict\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "7d7b_m9M44NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel\n",
        "\n",
        "model_id = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Load a tokenizer to preprocess the text\n",
        "clip_tokenizer = CLIPTokenizerFast.from_pretrained(model_id)\n",
        "\n",
        "# Load a processor to preprocess the images\n",
        "clip_processor = CLIPProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Main model for generating text and image embeddings\n",
        "model = CLIPModel.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "hWsq70tL4Gm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the embeddings\n",
        "text_embedding /= text_embedding.norm(dim=-1, keepdim=True)\n",
        "image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Calculate their similarity\n",
        "text_embedding = text_embedding.detach().cpu().numpy()\n",
        "image_embedding = image_embedding.detach().cpu().numpy()\n",
        "score = text_embedding @ image_embedding.T\n",
        "score"
      ],
      "metadata": {
        "id": "ktKyHoO74Gh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from PIL import Image\n",
        "\n",
        "# Load an AI-generated image of a puppy playing in the snow\n",
        "cat_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/cat.png\"\n",
        "car_path = \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-Language-Models/main/chapter09/images/car.png\"\n",
        "paths = [puppy_path, cat_path, car_path]\n",
        "images = [Image.open(urlopen(path)).convert(\"RGBA\") for path in paths]\n",
        "captions = [\n",
        "    \"a puppy playing in the snow\",\n",
        "    \"a pixelated image of a cute cat\",\n",
        "    \"A supercar on the road \\nwith the sunset in the background\"\n",
        "]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Embed all images\n",
        "image_embeddings = []\n",
        "for image in images:\n",
        "  image_processed = clip_processor(images=image, return_tensors='pt')['pixel_values']\n",
        "  image_embedding = model.get_image_features(image_processed).detach().cpu().numpy()[0]\n",
        "  image_embeddings.append(image_embedding)\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "\n",
        "# Embed all captions\n",
        "text_embeddings = []\n",
        "for caption in captions:\n",
        "  inputs = clip_tokenizer(caption, return_tensors=\"pt\")\n",
        "  text_emb = model.get_text_features(**inputs).detach().cpu().numpy()[0]\n",
        "  text_embeddings.append(text_emb)\n",
        "text_embeddings = np.array(text_embeddings)"
      ],
      "metadata": {
        "id": "hN-KsBVq4GOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create base figure\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(sim_matrix, cmap='viridis')\n",
        "\n",
        "# Adjust ticks with correct labels\n",
        "plt.yticks(range(len(captions)), captions, fontsize=18)\n",
        "plt.xticks([])\n",
        "\n",
        "# Visualize\n",
        "for i, image in enumerate(images):\n",
        "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
        "\n",
        "# Add the captions at the correct indices\n",
        "for x in range(sim_matrix.shape[1]):\n",
        "    for y in range(sim_matrix.shape[0]):\n",
        "        plt.text(x, y, f\"{sim_matrix[y, x]:.2f}\", ha=\"center\", va=\"center\", size=30)\n",
        "\n",
        "# Remove unnecessary spines\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "# Resize blocks\n",
        "plt.xlim([-0.5, len(captions) - 0.5])\n",
        "plt.ylim([len(captions) + 0.5, -2])\n",
        "# plt.title(\"Similarity Matrix\", size=20)\n",
        "plt.savefig(\"sim_matrix.png\", dpi=300, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "gu6O4NZtn9OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, Blip2ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load processor and main model\n",
        "blip_processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-opt-2.7b\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Send the model to GPU to speed up inference\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "k07yfRGh-LT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load SBERT-compatible CLIP model\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "\n",
        "# Encode the images\n",
        "image_embeddings = model.encode(images)\n",
        "\n",
        "# Encode the captions\n",
        "text_embeddings = model.encode(captions)\n",
        "\n",
        "#Compute cosine similarities\n",
        "sim_matrix = util.cos_sim(image_embeddings, text_embeddings)\n",
        "print(sim_matrix)"
      ],
      "metadata": {
        "id": "s5tWk4xU-Lta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"base_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "metadata": {
        "id": "IuOTTcGawkfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    output_dir=\"base_embedding_model\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    fp16=True,\n",
        "    eval_steps=100,\n",
        "    logging_steps=100,\n",
        ")"
      ],
      "metadata": {
        "id": "WiN--YWqwka0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkkFibOnRc8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ht_Sb2UwRc3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}